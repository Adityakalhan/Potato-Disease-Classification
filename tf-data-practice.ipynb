{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34a7c559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5eeeaad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_sales_number = [21,31,-13,43,12,15,13,75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a705f449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_dataset = tf.data.Dataset.from_tensor_slices(daily_sales_number)\n",
    "tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "22955a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "31\n",
      "-13\n",
      "43\n",
      "12\n",
      "15\n",
      "13\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "for sales in tf_dataset :\n",
    "    print(sales.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef6a3a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales numbers cannot be -ve\n",
    "tf_dataset_filtered = tf_dataset.filter(lambda x : x > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fe268227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "31\n",
      "43\n",
      "12\n",
      "15\n",
      "13\n",
      "75\n"
     ]
    }
   ],
   "source": [
    "for sales in tf_dataset_filtered :\n",
    "    print(sales.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "705f34d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1722\n",
      "2542\n",
      "3526\n",
      "984\n",
      "1230\n",
      "1066\n",
      "6150\n"
     ]
    }
   ],
   "source": [
    "tf_dataset_filtered = tf_dataset_filtered.map(lambda x : x * 82) #to convert to rs\n",
    "for sales in tf_dataset_filtered :\n",
    "    print(sales.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "846ee54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1722 2542]\n",
      "[3526  984]\n",
      "[1230 1066]\n",
      "[6150]\n"
     ]
    }
   ],
   "source": [
    "for sales_batch in tf_dataset_filtered.batch(2) : #doing batching because in my project will probably have to use batching for training\n",
    "    print(sales_batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83cb2dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2542 3526]\n",
      "[ 984 1722]\n",
      "[1066 1230]\n",
      "[6150]\n"
     ]
    }
   ],
   "source": [
    "## combining everything together\n",
    "tf_dataset = tf_dataset.filter(lambda x : x > 0).map(lambda y : y * 82).shuffle(2).batch(2)\n",
    "for sales in tf_dataset :\n",
    "    print(sales.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19c941ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.101.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.7/65.7 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.1.1-py3-none-any.whl (370 kB)\n",
      "     ------------------------------------ 370.9/370.9 kB 190.7 kB/s eta 0:00:00\n",
      "Collecting starlette<0.28.0,>=0.27.0\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "     -------------------------------------- 67.0/67.0 kB 303.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\aditya kalhan\\anaconda3\\lib\\site-packages (from fastapi) (4.5.0)\n",
      "Collecting pydantic-core==2.4.0\n",
      "  Downloading pydantic_core-2.4.0-cp39-none-win_amd64.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 63.8 kB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Downloading annotated_types-0.5.0-py3-none-any.whl (11 kB)\n",
      "Collecting typing-extensions>=4.5.0\n",
      "  Using cached typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in c:\\users\\aditya kalhan\\anaconda3\\lib\\site-packages (from starlette<0.28.0,>=0.27.0->fastapi) (3.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\aditya kalhan\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (3.4)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\aditya kalhan\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (1.2.0)\n",
      "Installing collected packages: typing-extensions, annotated-types, starlette, pydantic-core, pydantic, fastapi\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "Successfully installed annotated-types-0.5.0 fastapi-0.101.0 pydantic-2.1.1 pydantic-core-2.4.0 starlette-0.27.0 typing-extensions-4.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.10.0 requires grpcio<2.0,>=1.24.3, which is not installed.\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.2 which is incompatible.\n",
      "arviz 0.11.2 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.7.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73d39bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
      "     -------------------------------------- 59.5/59.5 kB 394.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.0 in c:\\users\\aditya kalhan\\anaconda3\\lib\\site-packages (from uvicorn) (4.7.1)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\aditya kalhan\\anaconda3\\lib\\site-packages (from uvicorn) (8.0.4)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\aditya kalhan\\anaconda3\\lib\\site-packages (from uvicorn) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aditya kalhan\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Installing collected packages: uvicorn\n",
      "Successfully installed uvicorn-0.23.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install uvicorn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b87634",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2828600531.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [1]\u001b[1;36m\u001b[0m\n\u001b[1;33m    docker pull tensorflow/serving\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "docker pull tensorflow/serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d226d40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
